\chapter{Derandomization}
In this section we will see a derandomization technique called Conditional Expectation. With this technique we will show derandomization of some randomized algorithms in the following sections.
\section{Conditional Expectation}
Let $\sA$ be a randomized algorithm which is successful with probability at least $\frac23$. Suppose $\sA$ uses $m$ random bits and suppose the random bits are $R_1,\dots , R_m$. Then we have $$\underset{R_1,\dots, R_m}{\bbP}[\sA(x,R_1,\dots, R_m)=\text{Correct}]\geq \frac23$$We want to derandomize $\sA$.

Now think of $\sA$ as a binary tree which, given $x$, branches on the sampled value of each random bit $R_i$ where it goes to left child if the random bit takes value $0$ and goes to right child if the random bit takes value $1$. Every path in this tree from root to leaf corresponds to different possible random strings and the leaf nodes corresponds to the output of the algorithm with the corresponding random string. Since $\sA$ succeeds with probability at least $\frac23$ means that at least $\frac23$ of the leaves are good outputs for the input $x$.

\begin{idea*}
	To derandomize $\sA$ we need to find a deterministic algorithm that traverses from the root to a leaf which at any branch at level $i$ chooses a direction which leads to a good output.
\end{idea*}

Now suppose $r_1,\dots, r_m\in \{0,1\}$ denote the values taken by the random variables $R_1,\dots, R_m$. Now let $P(r_1,\dots, r_i)$ denote the fraction of the leaves of the subtree below the node obtained  by following the path $r_1,\dots, r_i$. Formally, $$P(r_1,\dots, r_i)=\bbP[\sA(x,R_1,\dots, R_m)\mid R_1=r_1,\dots, R_i=r_i]=\frac12P(r_1,\dots, r_i,0)+\frac12P(r_1,\dots, r_i,1)$$From the last equality it is clear that there is a choice $r_{i+1}$ such that $P(r_1,\dots, r_{i+1})\geq P(r_1,\dots, r_i)$. Therefore to find a good path in the tree it suffices at each branch to pick such an $r\in \{0,1\}$. Then we would have $$P(r_1,\dots,r_m)\geq P(r_1,\dots, r_{m-1})\geq \cdots\geq P(r_1)\geq \bbP[\sA(x, R_1,\dots, R_m)=\text{Correct}]\geq \frac23$$Since $P(r_1,\dots, r_m)$ is either 0 or 1 it must be 1.
\section{\prb{Max-SAT}}
\begin{algoprob}
	\problemtitle{Max-SAT}
	\probleminput{SAT formula $\vph$ with $n$ variables and $m$ clauses and non negative weights $w_c$ on clauses.}
	\problemquestion{Given a SAT formula $\vph$ with $n$ variables and $m$ clauses and non negative weights $w_c$ on clauses find an assignment that maximizes weight of satisfied clauses.}
\end{algoprob}

We will first show a randomized algorithm for this problem. Then we will use conditional expectation to derandomize the algorithm.
\subsection{Randomized Algorithm}
First lets see what is the expected weight of satisfied clauses. Let $Y_c$ be the indicator random variable if clause $C$ is satisfied. Suppose there are $k$ variables in $C$. Then we have $\bbE[Y_c]=1-\frac1{2^k}\geq \frac12$. Therefore expected weight of satisfied clauses is $$\bbE\lt[\sum_{C}w_cY_c\rt]=\sum_Cw_c\bbE[Y_c]\geq \frac12\sum_Cw_c$$Let OPT be the optimal \prb{Max-SAT} solution for the given formula. Then we have $\sum\limits_Cw_c\geq \text{OPT}$.  Therefore $$\bbE\lt[\sum_{C}w_cY_c\rt]\geq \frac12\text{OPT}$$Hence we have the following randomized algorithm:

\begin{algorithm}
\DontPrintSemicolon
\KwIn{SAT formula $\vph$ with $n$ variables and $m$ clauses and non negative weights $w_c$ on clauses.}
\KwOut{Find an assignment that maximizes weight of satisfied clauses.}
\Begin{\For{$i\in[n]$}{$x_i\longleftarrow$ Pick a value from $\{0,1\}$ uniformly at random}
\Return{$x$}}
\caption{\prb{2-Approximate Max-SAT}}
\end{algorithm}

By the above discussion we have an assignment with an expected weight of satisfied clauses at least half the maximum.
\subsection{Derandomization}
Now we want to derandomize  the algorithm using conditional expectation. Let $X_1,\dots, X_n$ denote the random variable for each variables and $x_1,\dots, x_n\in \{0,1\}$ denote the value the random variables took. A key step will be evaluate the conditional probabilities: $$\bbE\lt[\sum_Cw_cY_c\mid X_1=x_1,\dots, X_i=x_i\rt]=\sum_Cw_c\bbP[Y_c=1\mid X_1=x_1,\dots, X_i=x_i]\quad \forall\ i\in[n]$$Hence we have to find the value of $\bbP[Y_c=1\mid X_1=x_1,\dots, X_i=x_i]$, $\forall\ i\in[n]$. Now if the clause $C$ is already satisfied by the setting $x_1,\dots, x_i$ then $Y_C=1$. Else if $C$ has $r$ variables from $x_{i+1},\dots, x_n$ then $$\bbP[Y_c=1\mid X_1=x_1,\dots, X_i=x_i]=1-\frac1{2^r}$$. Now if at height $i$, we find $\bbE\lt[\sum_Cw_cY_c\mid X_1=x_1,\dots, X_i=0\rt]$ and $\bbE\lt[\sum_Cw_cY_c\mid X_1=x_1,\dots, X_i=1\rt]$ and which ever gives the higher value we will set the assignment for $X_i$ to be that one. Thus we can derandomize the algorithm.
\section{Set Balancing}