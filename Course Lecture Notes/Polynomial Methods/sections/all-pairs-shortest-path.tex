\section{All Pairs Shortest Path}
\begin{algoprob}
	\problemtitle{All Pairs Shortest Path (APSP)}
	\probleminput{$G=(V,E)$ adjacency matrix, $V=[n]$. $G$ has no negative cycle.}
	\problemquestion{For all $u,v\in V$ output the length of the shortest path from $u$ to $v$ in $G$.}
\end{algoprob}

A very well known algorithm for solving this is  using dynamic programming.  Suppose \textsc{Shortest-Path}$(i,j,k)$= Length of shortest path $i$ to $j$ that only takes vertices in $[k]$. So we have the following relation $$\textsc{Shortest-Path}(i,j,k)=\min\lt\{  \textsc{Shortest-Path}(i,j,k-1), \textsc{Shortest-Path}(i,k,k-1)+\textsc{Shortest-Path}(k,j,k-1)  \rt\}$$The time complexity of the algorithm is $O(n^3)$.

\begin{theorem}{\cite{Williams_2014_Fap_CONF}}{apsp-wlliams}
	There is a randomized algorithm for \prb{APAP} with runtime $\frac{n^3}{2^{\veps\sqrt{\log n}}}$
\end{theorem}
 Before diving in to the randomized algorithm we will first look into another problem.


%\subsection{Min Plus Product of Matrices (MPP)}
\begin{algoprob}
	\problemtitle{Min Plus Product of Matrices (MPP)}
	\probleminput{$A,B\in\bbR^{n\times n}$}
	\problemquestion{$C=A\ostar B$ with $C(i,j)=\min\limits_k \{  A(i,k)+B(k,j)  \}$}
\end{algoprob}
Naively it takes $O(n^3)$ time to find the MPP of two matrices and no better algorithm with lesser time is known.
\begin{theorem}{}{}
	\begin{enumerate}
		\item \prb{APSP} is in time $T(n)\implies\prb{MPP}$ is in time $O(T(n))$.
		\item \prb{MPP} is in time $T(n)\implies \prb{APSP}$ is in time $O(T(n)\log n)$.
	\end{enumerate}
\end{theorem}
\begin{proof}
	\begin{enumerate}
		\item Let we have the matrices $A,B\in\bbR^{n\times n}$. Then we create the following 3-partite weighted graph $G(U,V,W,E)$ where $U=V=W=[n]$. For any $u\in U$, $v\in V$, $w\in W$ we have $w(u,v)=A(u,v)$ and $w(v,k)=B(v,k)$. \parinn
		
		For this graph we have that for any $i,j\in[n]$ $A\ostar B(i,j)=$ Length of the shortest path from $i\in U$ to $j\in W$. So now the problem is reduced to finding \prb{APSP} in the constructed graph and from that we can get the \prb{MPP} of the two matrices. So if we can solve \prb{APSP} in $T(n)$ time then we can also solve \prb{MPP} in $O(T(n))$ time.
		\item Let we have the adjacency matrix of a graph $G=(V,E)$ of $n$ vertices with self loops . Then we calculate the matrix $$\hat{A}=\underbrace{A\ostar A\ostar \cdots \ostar A}_{n\text{ times}}$$by \prb{MPP}. To calculate $\hat{A}$ efficiently we do repeated squaring to to have only $O(\log n)$ many \prb{MPP} operations. Each operation takes $T(n)$ time. Hence if \prb{MPP} can be solved in $T(n)$ time then \prb{APSP} can be solved in $O(T(n)\log n)$ time.
	\end{enumerate}
\end{proof}
Hence it is equivalent to show that there is an efficient algorithm for \prb{MPP} with which we can get an algorithm for \prb{APSP}

\begin{theorem}{\cite{Williams_2014_Fap_CONF}}{mpp-wlliams}
	There is a randomized algorithm for \prb{MPP} with runtime $\frac{n^3}{2^{\veps\sqrt{\log n}}}$
\end{theorem}
\begin{proof}
	So we have $A,B\in\bbR^{n\times n}$. We will break the algorithm into many steps to make it easy to understand.
	\begin{enumerate}[wide, label=\bfseries Step \arabic*,itemindent=0.5cm]
		\item \textbf{Reduction to Many Instances of Rectangular \prb{MPP}:} We first pick a parameter $s$ such that $s\mid n$. Let $g=\frac{n}{s}$. Then we break $A$ into $g$ many $n\times s$ blocks and we break $B$ into $g$ many $s\times n$ blocks i.e. $$\begin{array}{|c|c|c|c|}
			\hline
			& & & \\
		 & & & \\
			A_1 & A_2 & \cdots & A_g\\
			& & & \\
			& & & \\\hline
		\end{array}\ostar \begin{array}{|ccccccc|}
		\hline
		& & & B_1 & & &\\ \hline
		& & & B_2 & & &\\\hline
		& & & \vdots & & &\\\hline
		& & & B_g & & &\\\hline
	\end{array}$$
So now we define $C_i=A_i\ostar B_i$. Then we have for any $i,j\in[n]$, $C(i,j)=\min\limits_{j\in [g]}C_j$. Therefore it suffices to compute each $C_i$ in time $\tdO(n^2)$ time. From now on for brevity we will think $A$ to be a $n\times s$ matrix and $B$ to be a $s\times n$ matrix and we want to calculate the $C=A\ostar B$.
\item \textbf{Witness Matrix:} Suppose we have the matrix $W\in\bbR^{n\times n}$ such that for any $i,j\in[n]$ we have $$W(i,j)=k\text{ such that $k$ minimizes $A(i,k)+B(k,j)$, $k\in[s]$}$$Now having $W$ we can compute $C=A\ostar B$ in $O(n^2)$ time since for each $i,j\in[n]$ we get the index $k$ from $W(i,j)$ and then compute $A(i,k)+B(k,j)$ and it is equal to $C(i,j)$.\parinn

Therefore it suffices to compute $W$ in $O(n^2)$ time. Now $A,B$ may not have a unique witness matrix. Because for example suppose for some $i,j\in[n]$ there exists two indices $k,k'$ such that $A(i,k)+B(k,j)=A(i,k')+B(k',j)$ and they are minimum. Hence there might be instances where for multiple $k$ the value of $C$ is minimum at that point. So we need to make the witness matrix unique first.
\item \textbf{Making Witness Matrix Unique:} We have $A,B$ from Step 1. We create the following two new matrices $A',B'$ where for any $i,j\in[n]$ we have $$A'(i,j)=(n+1)A(i,j)+j\qquad B'(i,j)=(n+1)B(i,j)+j$$We claim that the corresponding witness matrix of $A',B'$ is uniquely defined.\parinn
\begin{claimwidth}
	\begin{Claim}{}{}
		Witness matrix $W'$ of $A',B'$ is witness matrix $W$ of $A,B$ and moreover each entry is uniquely defined.
	\end{Claim}
\begin{proof}
	Let for any $i,j\in[n]$, $W'(i,j)=k_1$ then we have $$A'(i,k_1)+B'(k_1,j)=(n+1)(A(i,k_1)+B(k_1,j))+k_1+j$$Suppose there exists another $k_2\in[s]$ such that $A'(i,k_2)+B'(k_2,j)=A'(i,k_1)+B'(k_1,j)$. Now two cases arise. \parinn
	
	If $k_1\neq k_2$ then either $A(i,k_1)+B(k_1,j)\neq A(i,k_2)+B(k_2,j)$ or $A(i,k_1)+B(k_1,j)= A(i,k_2)+B(k_2,j)$. If $A(i,k_1)+B(k_1,j)\neq A(i,k_2)+B(k_2,j)$ then we have $$(n+1)([A(i,k_1)+B(k_1,j)]-[A(i,k_2)+B(k_2,j)])=k_2-k_1>0$$ $k_2-k_1$ can never make for the $(n+1)$ since $(n+1)[A(i,k_1)+B(k_1,j)]$ and $(n+1)[ A(i,k_2)+B(k_2,j)]$ at least differ by a factor of $n+1$ and $k_2-k_1< n+1$. Else $k_1\neq k_2\implies A'(i,k_1)+B'(k_1,j)\neq A'(i,k_2)+B'(k_2,j)$.
	
	Therefore the witness of $A',B'$ is a witness of $A,B$ and its entries are uniquely defined
\end{proof}
\end{claimwidth}
Therefore with this we get an unique witness matrix $W'$. From now on for brevity we will call $A'$ by $A$, $B'$ by $B$ and $W'$ by $W$. 
\item \textbf{Fredman's Trick:} Now we have a witness matrix $W$ such that $\forall\ i,j\in[n]$ then $W(i,j)=$ unique $k\in[s]$. Now $\forall\ l\in [s]$ we have $$	A(i,k)+B(k,j) \leq A(i,l)+B(l,j)\iff A(i,k)-A(i,l)\leq B(l,j)-B(k,j)$$ So we create the matrices $$\tdA_i(k,l)=A(i,k)-A(i,l)\qquad \tdB_j(k,l)=B(l,j)-B(k,j)$$For $i\in[n]$, $\tdA_i$ and $\tdB_i$ are $s\times s$ matrices. 
	\end{enumerate}

\end{proof}